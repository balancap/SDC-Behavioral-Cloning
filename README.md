# Udacity Self Driving Car Nanodegree - Behavioral Cloning

Third project of the Udacity Nanodegree, and until now, the most challenging and fun one! What is this about? Basically, reimplement a simple version of the recent [article](https://arxiv.org/pdf/1604.07316v1.pdf) from Nvidia research team who described a neural network being able to perform end-to-end learning on self-driving cars. 

For that purpose, the Udacity team designed a simple simulator which let us acquire data (i.e. cameras, steering angle, speed and gas) and then test our solution of a self-driving neural network.

![](images/simulator.png "A Self-Driving car, how hard can it be?")

## Project Overview

Here are the main files constituting the project:
* `model.py`: Contains the model description and the basic code necessary for training;
* `drive.py`: Mainly Udacity code. Used to send the steering and gas commands to the simulator in self-driving mode. Just added some simple pre-processing to input image (normalization and cropping).
* `convert_img_to_data.py`: (Long) source in charge of pre-processing and augmenting the data generated by the simulator. It provides as output training data in numpy format (.npz files).
* `image_processing.py`: Custom improvements over the Keras implementation of the class `ImageDataGenerator`. I have added data flipping (image + steering angle) and additional color augmentation (random modification of contrast, brightness and saturation of input images).

## Data pre-processing

The pre-processing component of this project happened to be the most challenging part, partially due to my computer set-up. Like a few other students, I could only use my keyboard to drive and generate data on the simulator. As many have experienced, keyboard and game pad generate very different types of steering data, as shown by in the following graph (courtesy of Patrick Kern on Slack):

![](images/figure_kern.png "Keyboard vs Game Pad")

The keyboard output is clearly much more noisy and sparse compared to the smooth game pad signal. The former looks a lot like an impulse graph: most values are zeros whereas the remaining values are large pulse. As one may easily guess, this type of signal makes to the training procedure much more challenging, and often leads to a neural network being stucked during training in a non-optimal local minimum (typically, a constant steering angle).

Hence, I spend quite some time and effort trying to improve the steering angle data (even though far more time consuming, I found intellectually more challenging and interesting than buying a cheap Game Pad on Amazon !). A naive and simple approach to this problem could be to simply apply some smoothing filtering on the steering data (typically, exponential smoothing). As I discovered, the main problem with this simple solution is that the smooth output data does not necessarily make sense in terms of car physics. Namely, if one replace a one-pulse angle by an average over 5 or 10 frames, the car will not end up at the same position after these 5 / 10 frames. The physical system is clearly non-linear!

Consequently, one needs a more advanced strategy to tackle this smoothing problem. Inspired by the Nvidia team work, I deciced to take a more scientific and physical approach. The data provided by the simulator (steering angle, speed and frame rate) allows to re-construct (up to a certain precision) the trajectory of the car using known car physics (see this [page](http://www.asawicki.info/Mirror/Car%20Physics%20for%20Games/Car%20Physics%20for%20Games.html) for a reference on the topic). The algorithm, described in the function `trajectory` in the file `convert_img_to_data.py` is not very complex: for every frame, the angle allows to deduce the turning radius and displacement vectors, thus the next position. Updating the rotation matrix with this data, one can repeat this operation and obtain a re-construction of the trajectory.

![](images/trajectory.png "The drunk Udacity driver in action.")

As one can see on the picture above, the reconstruction is not perfect. If ones can recognise the shape of the track, the different laps do not overlap perfectly. There is an explanation for this error accumulating: I used a very simple physical model to simulate the car turning, which is only realistic at very low speed (as it does not take into account the momentum of the car), and thus does not match completely the more advanced model integrated in the simulator. 

## Network architecture and training

## Results
